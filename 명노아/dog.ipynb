{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강아지와 기정떡 구분하기\n",
    "1. 데이터 수집 : 강아지와 기정떡 데이터 수집하기.\n",
    "2. 데이터 전처리 : 사진 불러오기 (사진 크기 조절,  정규화)\n",
    "3. 데이터 분할 : 학습, 검증 및 테스트 데이터셋으로 나누기.\n",
    "4. 모델 설계 :\n",
    "5. 모델 학습 :\n",
    "6. 모델 평가 :\n",
    "7. 모델 테스트 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathn\\.conda\\envs\\Torch_PY38\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 지정된 프로시저를 찾을 수 없습니다'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# 모듈 불러오기.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 시 사용\n",
    "preprocessing = transforms.Compose(\n",
    "    [transforms.Resize(size=(50,50)),\n",
    "     transforms.ToTensor()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1-1] 이미지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 경로 설정\n",
    "train = './mini/'\n",
    "bichon = './mini/Bichon'\n",
    "zangidduk = './mini/zangidduk'\n",
    "#os.path.isdir(train), os.path.isdir(bichon), os.path.isdir(zangidduk) # 폴더 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bichon', 'zangidduk']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "imgFolder = ImageFolder(root=train, transform=preprocessing)\n",
    "imgFolder.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 1, 1, 1, 1, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 0, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 1, 0, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 0, 1, 1, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 1, 1, 0, 0, 0, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 0, 1, 1, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 1, 1, 0, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 0, 1, 0, 1, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 0, 1, 0, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 1, 1, 0, 0, 0, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 1, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 0, 0, 1, 0, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 0, 1, 0, 1, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 1, 1, 0, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 0, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 0, 1, 0, 0, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 0, 1, 0, 1, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 0, 1, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 1, 0, 1, 0, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 1, 0, 0, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 1, 1, 0, 1, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 1, 1, 0, 1, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 1, 1, 0, 0, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 1, 0, 0, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 1, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 0, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 0, 0, 1, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 1, 1, 0, 1, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 1, 0, 1, 0, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 1, 0, 1, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 0, 0, 0, 0, 1, 0, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 0, 0, 0, 1, 1])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([0, 0, 1, 1, 1, 0, 1, 0])\n",
      "torch.Size([8, 3, 50, 50]) torch.Size([8]) tensor([1, 1, 1, 0, 0, 0, 1, 1])\n",
      "torch.Size([2, 3, 50, 50]) torch.Size([2]) tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "imgDL = DataLoader(imgFolder, batch_size=8, shuffle=True)\n",
    "\n",
    "for (img, label) in imgDL :\n",
    "    print(img.shape, label.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module) :\n",
    "#     def __init__(self) :\n",
    "#         super().__init__()\n",
    "#         self.conLayer = nn.Conv2d(in_channels=3, out_channels=25, kernel_size = 3, stride = 1, padding = 0)\n",
    "#         self.conLayer2 = nn.Conv2d(in_channels=25, out_channels=10, kernel_size = 3, stride = 1, padding = 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[output] : torch.Size([1, 25, 4898, 7065]), [output2] : torch.Size([1, 3, 4896, 7063])\n"
     ]
    }
   ],
   "source": [
    "# conLayer 만들기\n",
    "conLayer = nn.Conv2d(in_channels=3, out_channels=25, kernel_size = 3, stride = 1, padding = 0)\n",
    "conLayer2 = nn.Conv2d(in_channels=25, out_channels=3, kernel_size = 3, stride = 1, padding = 0)\n",
    "output = conLayer(img)\n",
    "output2 = conLayer2(output)\n",
    "print(f'[output] : {output.shape}, [output2] : {output2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 4898, 7065]), torch.Size([1, 3, 4896, 7063]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#convLayer-> 활성화 함수 -> conLayer2 -> 활성화 함수 2\n",
    "relu1 = F.relu(input = output)\n",
    "relu2 = F.relu(input = conLayer2(relu1)) \n",
    "relu1.shape, relu2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 1632, 2354]), torch.Size([1, 3, 2447, 3531]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm1 = pool1(relu2)\n",
    "fm2 = pool2(relu2)\n",
    "fm1.shape, fm2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input] torch.Size([1, 3, 4900, 7067])\n",
      "[relu1] torch.Size([1, 25, 4898, 7065])\n",
      "[relu2] torch.Size([1, 3, 4896, 7063])\n",
      "[mp1] torch.Size([1, 3, 1632, 2354])\n"
     ]
    }
   ],
   "source": [
    "#Conv2 + Conv2D + Pooling\n",
    "print(f'[input] {img.shape}')\n",
    "output1 = conLayer(img)\n",
    "relu1 = F.relu(input=output1)\n",
    "print(f'[relu1] {relu1.shape}')\n",
    "\n",
    "output2 = conLayer2(relu1)\n",
    "relu2 = F.relu(input=output2)\n",
    "print(f'[relu2] {relu2.shape}')\n",
    "\n",
    "mp1 = pool1(relu2)\n",
    "print(f'[mp1] {mp1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
